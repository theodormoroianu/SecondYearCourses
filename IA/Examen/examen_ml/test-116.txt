1. Aplicati normalizarea L1 pe vectorul [4, 2, 5]. Care este vectorul cu normalizarea L2 aplicata?
A. [0.36, 0.18, 0.45]
B. [0.33, 0.16, 0.5]
C. [0.59, 0.29, 0.74]
D. [0.33, 0.27, 0.38]

2. Care este probabilitatea de aparitie clasei 3 avand urmatorul set de antrenare (exemplu, eticheta): [([1, 2, 3], 2), ([1, 4, 3], 1), ([1, 9, 3], 0), ([1, 2, 1], 1), ([11, 2, 3], 2)]
A. 0
B. 0.25
C. 0.12
D. 1

3. Daca avem probabilitatile pentru evenimentele P(A)=0.91, P(B)=0.82, P(B|A)=0.73. Calculati probabilitea P(A|B) folosind regula lui Bayes.
A. 0.65
B. 0.95
C. 0.81
D. 0.58

4. Cate ponderi invatabile (inclusiv bias) are un filtru convolutional de dimensiune 9x9x5?
A. 24
B. 405
C. 23
D. 406

5. Aplicati normalizarea L1 pe vectorul [4, 2, 5]. Care este vectorul cu normalizarea L1 aplicata?
A. [0.5, 0.0, 1.0]
B. [0.33, 0.27, 0.38]
C. [0.36, 0.18, 0.45]
D. [0.33, 0.16, 0.5]

6. Care este matricea de confuzie a unui clasificator, pentru setul de test cu etichetele [1,2,3,1,1,2,1,2,2,2] si predictiile [2,1,3,1,1,1,1,2,2,2]?
A. [[2 0 0],[1 4 1],[0 1 1]]
B. [[1 2 0],[0 3 0],[1 0 3]]
C. [[3 2 0],[1 3 0],[0 0 1]]
D. [[3 1 0],[2 3 0],[0 0 1]]

7. Care dintre urmatoarele metode foloseste conceptele de soft-margin si hard-margin?
A. Arbori de decizie
B. Metoda celor mai apropiati vecini
C. Masini cu vector suport
D. K-means clustering

8. Cat este valoarea functiei de pierdere hinge loss pentru y = [1, -1] si y_hat = [1.1, -0.7]?
A. -0.3
B. 0.4
C. 0.2
D. 0.3

9. Cati clasificatori sunt utilizati in antrenarea unui SVM cu abordare one vs one pentru urmatoarele date: [(2,2),(1,2),(1,1),(2,1),(1,1),(-1,-1),(1,-1),(-1,1)], cu etichetele: [1,2,1,4,2,3,4,3]?
A. 6
B. 8
C. 10
D. 5

10. In cazul unui model one-versus-all, care este principala diferenta in utilizarea functiei de pierdere hinge versus cross-entropy?
A. Cross-entropy permite continuarea optimizarii in cazul in care marginea dintre scoruri este prea mica
B. Cross-entropy permite continuarea optimizarii in cazul in care marginea dintre scoruri este suficient de mare
C. Hinge permite continuarea optimizarii in cazul in care marginea dintre scoruri este suficient de mare
D. Modele sunt in esenta echivalente

